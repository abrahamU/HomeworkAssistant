import streamlit as st
import json
import openai
from datetime import datetime
from openai import OpenAI

def run():
    # è®¾ç½® API ç«¯ç‚¹å’Œ OpenAI API å¯†é’¥
    client = OpenAI(api_key=st.secrets["1"], base_url=st.secrets["2"])

    response = False
    if 'Outline' not in st.session_state:
        st.session_state['Outline'] = ""
    if 'OutlineObj' not in st.session_state:
        st.session_state['OutlineObj'] = {}
    if 'sectionsList' not in st.session_state:
        st.session_state['sectionsList'] = []
    if 'finished' not in st.session_state:
        st.session_state['finished'] = False

    # è®¾ç½®æ ‡é¢˜
    st.title('ç®¡ç†ç¡•å£«ä½œä¸šå†™ä½œåŠ©æ‰‹')
    st.subheader('æ­¤åŠ©æ‰‹å¯ä»¥æ ¹æ®ä½ çš„æ’°å†™èŒƒå›´æ–¹å‘åŠå†™ä½œçº²è¦ä½œä¸šè¦æ±‚ï¼Œç”Ÿæˆ5000å­—ä»¥å†…çš„ä½œä¸šï¼ˆåŒ…å«é¢˜ç›®Â·æ­£æ–‡Â·å‚è€ƒæ–‡çŒ®ï¼‰')
    st.subheader('è¯·ä½ æ ¹æ®ä½ çš„ä½œä¸šéœ€æ±‚å¡«å†™ä¸€ä¸‹ä½œä¸šè¦æ±‚ï¼Œå¡«å†™çš„æ¯”è¾ƒè¯¦ç»†ç”Ÿæˆå†…å®¹è¶Šç¬¦åˆä½œä¸šè¦æ±‚')


    # ä¸ºç”¨æˆ·æä¾›è¾“å…¥æ¡†
    st.markdown('##### 1.è¯·è¾“å…¥ä½ çš„ä½œä¸šé¢˜ç›®ï¼Œæˆ–è€…è¾“å…¥ä½ çš„ä½œä¸šå†™ä½œæ–¹å‘ï¼Œæˆ‘å¸®ä½ ä½ ç”Ÿæˆä½œä¸šé¢˜ç›®ï¼ˆæ­¤å¤„æç»˜çš„è¦æ¸…æ¥šÂ·å¿…é¡»å¡«å†™ï¼‰ğŸ“œ')
    title = st.text_input('ä½œä¸šé¢˜ç›®')
    st.markdown('##### ä½œä¸šè¯­è¨€è¦æ±‚ ä¸­æ–‡ è‹±æ–‡ ï¼ˆå¿…å¡«ï¼‰')
    language = st.selectbox('ä½œä¸šè¯­è¨€',["ä¸­æ–‡","English"])
    st.markdown('##### ä½œä¸šå­—æ•°è¦æ±‚ï¼ˆå¿…å¡«ï¼‰')
    counts = st.text_input('ä½œä¸šå­—æ•°')
    st.markdown('##### ä½œä¸šç« èŠ‚è¦æ±‚ï¼Œç¬¬ä¸€ç« èŠ‚xxxxï¼Œç¬¬äºŒç« èŠ‚xxxxç­‰ ï¼ˆè‹¥ä½œä¸šæ²¡æœ‰ç›¸å…³è¦æ±‚å¯ä»¥ä¸æ·»ï¼‰ï¼Œæˆ‘å¸®ä½ ç”Ÿæˆç« èŠ‚')
    angle_and_conclusion = st.text_area('ç« èŠ‚ä»‹ç»ï¼ˆå¯é€‰ï¼‰')
    st.markdown('##### ä½œä¸šå…¶ä»–è¦æ±‚ï¼Œåœ¨è¿™é‡Œè¾“å…¥ï¼ˆå¦‚æœæ²¡æœ‰å¯ä»¥ä¸æ·»ï¼‰')
    additional_info = st.text_area('å…¶ä»–è¦æ±‚ï¼ˆå¯é€‰ï¼‰')

    # æä¾›ä¸€ä¸ªæŒ‰é’®ï¼Œç‚¹å‡»åç”Ÿæˆå¤§çº²
    if st.button('ç”Ÿæˆæ–‡ç« '):
        if not title or not language or not counts:
            # å¦‚æœç”¨æˆ·åæˆ–å¯†ç ä¸ºç©ºï¼Œåˆ™æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
            st.warning("è¯·å¡«å†™å¿…é¡»å¡«å†™çš„ä¿¡æ¯")
        else:
            messages = []
            GPT_response = st.empty()
            prompt = f"""
            ## ä»»åŠ¡
            æ ¹æ®ä»¥ä¸‹çš„ä¿¡æ¯è¿›è¡Œæ–‡ç« å¤§çº²çš„åˆ›ä½œï¼Œä¸ºè¯¥æ–‡ç« å–ä¸€ä¸ªé†’ç›®çš„åç§°ï¼Œå¹¶ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆä¸€äº›æ¦‚æ‹¬æ€§å†…å®¹
            1. **ä¸»é¢˜å’Œç„¦ç‚¹**ï¼š
                - æ–‡ç« çš„æ ‡é¢˜å’ŒèŒƒå›´ï¼š{title}
                - ä½œä¸šçš„è¦æ±‚ {angle_and_conclusion}
                - æ˜¯å¦æœ‰ä¸€äº›é™„åŠ ä¿¡æ¯ä¸ºæˆ‘æä¾›æœ€æ–°çš„å†…å®¹? {additional_info}
            2. **å†™ä½œä»»åŠ¡**ï¼š
                - æ–‡ç« çš„ç±»å‹ï¼Ÿè¿™æ˜¯ä¸€ç¯‡è®ºæ–‡ä½œä¸š
                - ä½ å¸Œæœ›è®©è¯»è€…é€šè¿‡æ–‡ç« è·å¾—ä»€ä¹ˆæ ·çš„ä½“éªŒæˆ–ä¿¡æ¯ï¼Ÿå¸Œæœ›èƒ½å¤Ÿè®ºè¯è‡ªå·±çš„è§‚ç‚¹
                - è¾“å‡ºçš„è¯­è¨€ä¸º {language}
            3. **æ–‡ç« é£æ ¼**ï¼š
                - ä½ å¸Œæœ›æ–‡ç« çš„è¯­è°ƒæˆ–é£æ ¼æ˜¯æ€æ ·çš„ï¼Ÿå­¦æœ¯
            4. **æ ¼å¼å’Œç»“æ„**ï¼š
                - å¼•è¨€ // å¿…é¡»è¦æœ‰ï¼Œå¯¹å…¨æ–‡åšä¸€ä¸ªæ€»ç»“æ€§çš„è¯´æ˜
                - 3-4ä¸ªç« èŠ‚ // æ ¹æ®æ–‡ç« çš„å­—æ•°{counts}ï¼Œæ¥æ„å»ºç« èŠ‚ï¼Œ3000ä»¥ä¸Šä¸º3ç« ï¼Œ4000å­—ä»¥ä¸Šä¸º4ç« 
                - ç»“è®º // å¿…é¡»è¦æœ‰ï¼Œå¯¹å…¨æ–‡çš„ç»“è®ºåšä¸€ä¸ªè¯´æ˜
                - å‚è€ƒæ–‡çŒ® // **å‚è€ƒæ–‡çŒ®å¿…é¡»çœŸå®æœ‰æ•ˆï¼Œå¹¶æœ‰æƒå¨çš„æ¥æº**ï¼Œæ¯”å¦‚google scholarsæˆ–è€…ç™¾åº¦æ–‡çŒ®
            """
            # test = f"é²è¿…å’Œå‘¨æ ‘äººçš„å…³ç³»"
            with st.spinner("æ­£åœ¨ç”Ÿæˆå¤§çº²ï¼Œè¯·ç¨å€™..."):
                chat_completion = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡ç« å†™æ‰‹,ä»¥markdownæ ¼å¼è¾“å‡º"},
                        {"role": "user", "content": prompt},
                        ],
                    stream=True,
                )

                for token in chat_completion:
                    content = token.choices[0].delta.content # new API result
                    if content != None:
                        messages.append(content)
                        full_reply_content = ''.join([m for m in messages])
                        GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")

                st.session_state['Outline'] = full_reply_content
                st.rerun()

        if st.session_state['Outline']:
            #st.write(st.session_state['Outline'])
            if st.session_state['finished']:
                st.write(st.session_state['Outline'])
                st.write(st.session_state['OutlineObj'])
                st.write(st.session_state['sectionsList'])

                chat_text = ""
                chat_text += f"# {st.session_state['OutlineObj']['title']} \n"
                sectionsList = st.session_state['sectionsList']
                i=0

                while i < len(sectionsList):
                    chat_text += f"## {st.session_state['OutlineObj']['sections'][i]} \n"
                    chat_text += f"{sectionsList[i]}\n"
                    i = i+1
                
                references = st.session_state['OutlineObj']['references']
                j=0

                chat_text += f"## å‚è€ƒæ–‡çŒ®\n"
                while j < len(references):
                    chat_text += f"{j+1}. {st.session_state['OutlineObj']['references'][j]} \n"
                    j = j+1
                        
                # æ·»åŠ ä¸‹è½½æŒ‰é’®
                timestamp = datetime.now()
                file_name = f"Article_records_{timestamp}.txt"
                st.download_button(
                    label="ä¸‹è½½æ–‡ç« ",
                    data=chat_text,
                    file_name=file_name,
                    mime="text/plain"
                    )
            
            else:
                if st.session_state['OutlineObj']:
                    st.write(st.session_state['OutlineObj'])
                    st.write("å¼€å§‹æŒ‰ç…§ç« èŠ‚ç›®å½•æ¥è¿›è¡Œå†…å®¹çš„ç”Ÿæˆ")
                    title = st.session_state['OutlineObj']["title"]
                    sections = st.session_state['OutlineObj']["sections"]
                    for section in sections:
                        msg = f"æ‰©å†™æ ‡é¢˜ï¼š{title}ä¸‹çš„ç« èŠ‚:{section},è¯·ç¨å..."
                        with st.spinner(msg):
                            messages = []
                            GPT_response = st.empty()

                            prompt = f"""
                            # MBAä½œä¸šåŠ©æ‰‹-æ‰©å†™
                            ## ä»»åŠ¡
                            1. åŸºäº<å¤§çº²>, ç»“åˆæ ‡é¢˜ {title} ,**ä»…å¯¹ {section} éƒ¨åˆ†è¿›è¡Œæ‰©å†™**ï¼Œéœ€è¦è‡³å°‘1000æ±‰å­—çš„å†…å®¹
                            1.1 **æ³¨æ„ä¸éœ€è¦è¾“å‡ºç« èŠ‚çš„æ ‡é¢˜ï¼ï¼**
                            1.2 **æ³¨æ„ä¸éœ€è¦ç›´æ¥è¾“å‡ºå‚è€ƒæ–‡çŒ®ï¼ï¼ä½†éœ€è¦åœ¨åŸæ–‡ä¸­åˆ©ç”¨è§’æ ‡è¿›è¡Œå‚è€ƒæ–‡çŒ®çš„æ ‡æ³¨**
                            2. ç»“åˆ<æ³¨æ„äº‹é¡¹>è¿›è¡Œå†…å®¹çš„è¾“å‡º
                            ## å¤§çº²
                            {st.session_state['Outline']}
                            ## æ³¨æ„äº‹é¡¹
                            1. **ç›®æ ‡å—ä¼—**ï¼š
                                - è¿™æ˜¯ä¸€ç¯‡è®ºæ–‡ä½œä¸š
                                - å¸Œæœ›èƒ½å¤Ÿè®ºè¯è‡ªå·±çš„è§‚ç‚¹
                            2. **æ–‡ç« é£æ ¼**ï¼š
                                - å­¦æœ¯ //æ³¨æ„ï¼Œä¸è¦ä»¥â€˜æˆ‘ä»¬â€™çš„å£å»è¿›è¡Œè¾“å‡ºï¼Œè¦çªå‡ºä½ çš„è´¡çŒ®
                            3. **å›¾è¡¨æˆ–è§†è§‰å…ƒç´ **ï¼š
                                - åœ¨éœ€è¦è¿›è¡Œæ’å…¥è§†è§‰å…ƒç´ çš„åœ°æ–¹è¿›è¡Œæ ‡æ³¨
                            4. **å‚è€ƒæ–‡çŒ®**ï¼š
                                - **ä¸éœ€è¦è¾“å‡ºå‚è€ƒæ–‡çŒ®ï¼Œä½†éœ€è¦åœ¨åŸæ–‡ä¸­åˆ©ç”¨è§’æ ‡è¿›è¡Œå‚è€ƒæ–‡çŒ®çš„æ ‡æ³¨**
                            """

                            chat_completion = client.chat.completions.create(
                                model="gpt-4-1106-preview",
                                messages=[
                                    {"role": "system", "content": ""},
                                    {"role": "user", "content": prompt}
                                ],
                                stream=True,
                            )

                            if isinstance(chat_completion, dict):
                                # not stream
                                st.write(chat_completion.choices[0].message.content)
                            else:
                                # stream
                                for token in chat_completion:
                                    content = token.choices[0].delta.content # new API result
                                    if content != None:
                                        messages.append(content)
                                        full_reply_content = ''.join([m for m in messages])
                                        GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")

                                st.session_state['sectionsList'].append(full_reply_content)                       
                    st.session_state['finished'] = True
                    st.rerun()
                
                else:
                    with st.spinner("æ­£åœ¨åˆ†æå¤§çº²å†…å®¹ï¼Œè¯·ç¨å€™..."):
                        messages = []
                        GPT_response = st.empty()

                        prompt = f"""
                            # OutlineToJson
                            ## Task
                            * READ THE <outline> and OUTPUT A JSON STRING ONLY!!!.without any markdown symbol
                            * the Json String is DEFINED by <JSON Format>
                            * MAKE SURE your output can be pharsed by json.load() directly!
                            ### JSON Format
                            {{
                                "title": "æ–‡ç« æ ‡é¢˜",
                                "sections": [], //åªéœ€è¦è¿”å›ç« èŠ‚ï¼ˆåŒ…å«å¼•è¨€å’Œç»“è®ºï¼‰çš„titleå³å¯ï¼Œä¸éœ€è¦è¿”å›ç« èŠ‚å†…å®¹ï¼ï¼ï¼
                                "references":[], //å°†å‚è€ƒæ–‡çŒ®ä¿å­˜ä¸ºä¸€ä¸ªåˆ—è¡¨
                            }}
                            ### outline
                            {st.session_state['Outline']}
                        """

                        chat_completion = client.chat.completions.create(
                            model="gpt-4-1106-preview",
                            response_format={"type": "json_object"},
                            messages=[
                                {"role": "system", "content": ""},
                                {"role": "user", "content": prompt}
                                ],
                            stream=True,
                        )

                        if isinstance(chat_completion, dict):
                            # not stream
                            st.write(chat_completion.choices[0].message.content)
                        else:
                            # stream
                            for token in chat_completion:
                                content = token.choices[0].delta.content # new API result
                                if content != None:
                                    messages.append(content)
                                    full_reply_content = ''.join([m for m in messages])
                                    GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")
                            # jsonstring=full_reply_content.replace('```json','')
                            st.write(full_reply_content)
                            st.session_state['OutlineObj'] = json.loads(full_reply_content)
                            st.rerun()