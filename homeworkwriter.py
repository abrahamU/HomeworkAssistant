import streamlit as st
import json
import openai
from datetime import datetime
from openai import OpenAI

def run():
    # è®¾ç½® API ç«¯ç‚¹å’Œ OpenAI API å¯†é’¥
    client = OpenAI(api_key=st.secrets["1"], base_url=st.secrets["2"])

    response = False
    if 'Outline' not in st.session_state:
        st.session_state['Outline'] = ""
    if 'OutlineObj' not in st.session_state:
        st.session_state['OutlineObj'] = {}
    if 'sectionsList' not in st.session_state:
        st.session_state['sectionsList'] = []
    if 'finished' not in st.session_state:
        st.session_state['finished'] = False

    # è®¾ç½®æ ‡é¢˜
    st.title('é•¿æ–‡æœ¬åŠ©æ‰‹ - MBAä½œä¸šå†™ä½œ')

    # ä¸ºç”¨æˆ·æä¾›è¾“å…¥æ¡†
    st.markdown('##### è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³ç”Ÿæˆçš„æ–‡ç« ä¸»é¢˜ï¼Œæˆ‘ä¼šä¸ºæ‚¨æ„å»ºä¸€ä¸ªåˆé€‚çš„é¢˜ç›®ğŸ“œ')
    title = st.text_input('æ–‡ç« çš„ä¸»é¢˜')
    st.markdown('##### å¦‚æœæ‚¨èƒ½å¤Ÿå‘Šè¯‰æˆ‘ä¸€äº›æ‚¨æ‰€èšç„¦çš„è§’åº¦å·²ç»æ‚¨çš„è§‚ç‚¹ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨ç”Ÿæˆæ›´ç¬¦åˆé¢„æœŸçš„å†…å®¹ğŸ˜Š')
    angle_and_conclusion = st.text_input('èšç„¦çš„è§’åº¦å’Œç‰¹å®šçš„ç»“è®º')
    st.markdown('##### ç”±äºæˆ‘çš„çŸ¥è¯†æœ‰é™ï¼Œæ‚¨å¯ä»¥å‘Šè¯‰æˆ‘ä¸€äº›å…³äºè¯¥ä¸»é¢˜çš„æœ€æ–°æ¶ˆæ¯ï¼Œæˆ‘ä¼šç»“åˆè¿™äº›å†…å®¹ç”Ÿæˆæ›´ç¬¦åˆå½“ä¸‹æƒ…å†µçš„å†…å®¹ğŸ˜œ')
    additional_info = st.text_area('é™„åŠ çš„ä¿¡æ¯')

    # æä¾›ä¸€ä¸ªæŒ‰é’®ï¼Œç‚¹å‡»åç”Ÿæˆå¤§çº²
    if st.button('ç”Ÿæˆæ–‡ç« '):
        messages = []
        GPT_response = st.empty()
        prompt = f"""
        ## ä»»åŠ¡
        æ ¹æ®ä»¥ä¸‹çš„ä¿¡æ¯è¿›è¡Œæ–‡ç« å¤§çº²çš„åˆ›ä½œï¼Œä¸ºè¯¥æ–‡ç« å–ä¸€ä¸ªé†’ç›®çš„åç§°ï¼Œå¹¶ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆä¸€äº›æ¦‚æ‹¬æ€§å†…å®¹
        1. **ä¸»é¢˜å’Œç„¦ç‚¹**ï¼š
            - ä½ å¸Œæœ›æ–‡ç« å…³æ³¨ä»€ä¹ˆä¸»é¢˜ï¼Ÿ{title}
            - æ˜¯å¦æœ‰ç‰¹å®šçš„ç„¦ç‚¹æˆ–è§’åº¦ä½ å¸Œæœ›æ¢è®¨ï¼Ÿ{angle_and_conclusion}
            - æ˜¯å¦æœ‰ä¸€äº›é™„åŠ ä¿¡æ¯ä¸ºæˆ‘æä¾›æœ€æ–°çš„å†…å®¹? {additional_info}
        2. **å†™ä½œä»»åŠ¡**ï¼š
            - æ–‡ç« çš„ç±»å‹ï¼Ÿè¿™æ˜¯ä¸€ç¯‡å†™ç»™MBAæ•™æˆçš„ä½œä¸š
            - ä½ å¸Œæœ›è®©è¯»è€…é€šè¿‡æ–‡ç« è·å¾—ä»€ä¹ˆæ ·çš„ä½“éªŒæˆ–ä¿¡æ¯ï¼Ÿå¸Œæœ›èƒ½å¤Ÿè®ºè¯è‡ªå·±çš„è§‚ç‚¹
        3. **æ–‡ç« é£æ ¼**ï¼š
            - ä½ å¸Œæœ›æ–‡ç« çš„è¯­è°ƒæˆ–é£æ ¼æ˜¯æ€æ ·çš„ï¼Ÿå­¦æœ¯
        4. **å­—æ•°è¦æ±‚**ï¼š
            - æ–‡ç« åº”åŒ…å«å¤šå°‘å­—ï¼Ÿä½ èƒ½è¾“å‡ºå¤šå°‘å°±è¾“å‡ºå¤šå°‘
            - æ˜¯å¦æœ‰æœ€å°æˆ–æœ€å¤§å­—æ•°çš„é™åˆ¶ï¼Ÿä½ èƒ½è¾“å‡ºå¤šå°‘å°±è¾“å‡ºå¤šå°‘
        5. **æ ¼å¼å’Œç»“æ„**ï¼š
            - å¼•è¨€ // å¿…é¡»è¦æœ‰ï¼Œå¯¹å…¨æ–‡åšä¸€ä¸ªæ€»ç»“æ€§çš„è¯´æ˜
            - 3-4ä¸ªç« èŠ‚ // è¯·ç›´æ¥ç”Ÿæˆç« èŠ‚åç§°
            - ç»“è®º // å¿…é¡»è¦æœ‰ï¼Œå¯¹å…¨æ–‡çš„ç»“è®ºåšä¸€ä¸ªè¯´æ˜
            - å‚è€ƒæ–‡çŒ® // **å‚è€ƒæ–‡çŒ®å¿…é¡»çœŸå®æœ‰æ•ˆï¼Œå¹¶æœ‰æƒå¨çš„æ¥æº**ï¼Œæ¯”å¦‚google scholarsæˆ–è€…ç™¾åº¦æ–‡çŒ®
        """
        # test = f"é²è¿…å’Œå‘¨æ ‘äººçš„å…³ç³»"
        with st.spinner("æ­£åœ¨ç”Ÿæˆå¤§çº²ï¼Œè¯·ç¨å€™..."):
            chat_completion = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡ç« å†™æ‰‹,ä»¥markdownæ ¼å¼è¾“å‡º"},
                    {"role": "user", "content": prompt},
                    ],
                stream=True,
            )

            for token in chat_completion:
                content = token.choices[0].delta.content # new API result
                if content != None:
                    messages.append(content)
                    full_reply_content = ''.join([m for m in messages])
                    GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")

            st.session_state['Outline'] = full_reply_content
            st.rerun()

    if st.session_state['Outline']:
        #st.write(st.session_state['Outline'])
        if st.session_state['finished']:
            st.write(st.session_state['Outline'])
            st.write(st.session_state['OutlineObj'])
            st.write(st.session_state['sectionsList'])

            chat_text = ""
            chat_text += f"# {st.session_state['OutlineObj']['title']} \n"
            sectionsList = st.session_state['sectionsList']
            i=0

            while i < len(sectionsList):
                chat_text += f"## {st.session_state['OutlineObj']['sections'][i]} \n"
                chat_text += f"{sectionsList[i]}\n"
                i = i+1
            
            references = st.session_state['OutlineObj']['references']
            j=0

            chat_text += f"## å‚è€ƒæ–‡çŒ®\n"
            while j < len(references):
                chat_text += f"{j+1}. {st.session_state['OutlineObj']['references'][j]} \n"
                j = j+1
                    
            # æ·»åŠ ä¸‹è½½æŒ‰é’®
            timestamp = datetime.now()
            file_name = f"Article_records_{timestamp}.txt"
            st.download_button(
                label="ä¸‹è½½æ–‡ç« ",
                data=chat_text,
                file_name=file_name,
                mime="text/plain"
                )
        
        else:
            if st.session_state['OutlineObj']:
                st.write(st.session_state['OutlineObj'])
                st.write("å¼€å§‹æŒ‰ç…§ç« èŠ‚ç›®å½•æ¥è¿›è¡Œå†…å®¹çš„ç”Ÿæˆ")
                title = st.session_state['OutlineObj']["title"]
                sections = st.session_state['OutlineObj']["sections"]
                for section in sections:
                    msg = f"æ‰©å†™æ ‡é¢˜ï¼š{title}ä¸‹çš„ç« èŠ‚:{section},è¯·ç¨å..."
                    with st.spinner(msg):
                        messages = []
                        GPT_response = st.empty()

                        prompt = f"""
                        # MBAä½œä¸šåŠ©æ‰‹-æ‰©å†™
                        ## ä»»åŠ¡
                        1. åŸºäº<å¤§çº²>, ç»“åˆæ ‡é¢˜ {title} ,**ä»…å¯¹ {section} éƒ¨åˆ†è¿›è¡Œæ‰©å†™**ï¼Œéœ€è¦è‡³å°‘1000æ±‰å­—çš„å†…å®¹
                        1.1 **æ³¨æ„ä¸éœ€è¦è¾“å‡ºç« èŠ‚çš„æ ‡é¢˜ï¼ï¼**
                        1.2 **æ³¨æ„ä¸éœ€è¦ç›´æ¥è¾“å‡ºå‚è€ƒæ–‡çŒ®ï¼ï¼ä½†éœ€è¦åœ¨åŸæ–‡ä¸­åˆ©ç”¨è§’æ ‡è¿›è¡Œå‚è€ƒæ–‡çŒ®çš„æ ‡æ³¨**
                        2. ç»“åˆ<æ³¨æ„äº‹é¡¹>è¿›è¡Œå†…å®¹çš„è¾“å‡º
                        ## å¤§çº²
                        {st.session_state['Outline']}
                        ## æ³¨æ„äº‹é¡¹
                        1. **ç›®æ ‡å—ä¼—**ï¼š
                            - MBAæ•™æˆ // ä½†ä¸è¦åœ¨å†…å®¹ä¸­ç›´æ¥æåˆ°è¿™ä¸ªä¿¡æ¯
                            - å¸Œæœ›èƒ½å¤Ÿè®ºè¯è‡ªå·±çš„è§‚ç‚¹
                        2. **æ–‡ç« é£æ ¼**ï¼š
                            - å­¦æœ¯ //æ³¨æ„ï¼Œä¸è¦ä»¥â€˜æˆ‘ä»¬â€™çš„å£å»è¿›è¡Œè¾“å‡ºï¼Œè¦çªå‡ºä½ çš„è´¡çŒ®
                        3. **å›¾è¡¨æˆ–è§†è§‰å…ƒç´ **ï¼š
                            - åœ¨éœ€è¦è¿›è¡Œæ’å…¥è§†è§‰å…ƒç´ çš„åœ°æ–¹è¿›è¡Œæ ‡æ³¨
                        4. **å‚è€ƒæ–‡çŒ®**ï¼š
                            - **ä¸éœ€è¦è¾“å‡ºå‚è€ƒæ–‡çŒ®ï¼Œä½†éœ€è¦åœ¨åŸæ–‡ä¸­åˆ©ç”¨è§’æ ‡è¿›è¡Œå‚è€ƒæ–‡çŒ®çš„æ ‡æ³¨**
                        """

                        chat_completion = client.chat.completions.create(
                            model="gpt-4-1106-preview",
                            messages=[
                                {"role": "system", "content": ""},
                                {"role": "user", "content": prompt}
                            ],
                            stream=True,
                        )

                        if isinstance(chat_completion, dict):
                            # not stream
                            st.write(chat_completion.choices[0].message.content)
                        else:
                            # stream
                            for token in chat_completion:
                                content = token.choices[0].delta.content # new API result
                                if content != None:
                                    messages.append(content)
                                    full_reply_content = ''.join([m for m in messages])
                                    GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")

                            st.session_state['sectionsList'].append(full_reply_content)                       
                st.session_state['finished'] = True
                st.rerun()
            
            else:
                with st.spinner("æ­£åœ¨åˆ†æå¤§çº²å†…å®¹ï¼Œè¯·ç¨å€™..."):
                    messages = []
                    GPT_response = st.empty()

                    prompt = f"""
                        # OutlineToJson
                        ## Task
                        * READ THE <outline> and OUTPUT A JSON STRING ONLY!!!.without any markdown symbol
                        * the Json String is DEFINED by <JSON Format>
                        * MAKE SURE your output can be pharsed by json.load() directly!
                        ### JSON Format
                        {{
                            "title": "æ–‡ç« æ ‡é¢˜",
                            "sections": [], //åªéœ€è¦è¿”å›ç« èŠ‚ï¼ˆåŒ…å«å¼•è¨€å’Œç»“è®ºï¼‰çš„titleå³å¯ï¼Œä¸éœ€è¦è¿”å›ç« èŠ‚å†…å®¹ï¼ï¼ï¼
                            "references":[], //å°†å‚è€ƒæ–‡çŒ®ä¿å­˜ä¸ºä¸€ä¸ªåˆ—è¡¨
                        }}
                        ### outline
                        {st.session_state['Outline']}
                    """

                    chat_completion = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        response_format={"type": "json_object"},
                        messages=[
                            {"role": "system", "content": ""},
                            {"role": "user", "content": prompt}
                            ],
                        stream=True,
                    )

                    if isinstance(chat_completion, dict):
                        # not stream
                        st.write(chat_completion.choices[0].message.content)
                    else:
                        # stream
                        for token in chat_completion:
                            content = token.choices[0].delta.content # new API result
                            if content != None:
                                messages.append(content)
                                full_reply_content = ''.join([m for m in messages])
                                GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")
                        # jsonstring=full_reply_content.replace('```json','')
                        st.write(full_reply_content)
                        st.session_state['OutlineObj'] = json.loads(full_reply_content)
                        st.rerun()