import streamlit as st
import json
import openai
from datetime import datetime
from openai import OpenAI
import pypandoc
from io import BytesIO

def run():
    # è®¾ç½® API ç«¯ç‚¹å’Œ OpenAI API å¯†é’¥
    client = OpenAI(api_key=st.secrets["1"], base_url=st.secrets["2"])

    response = False
    if 'Outline' not in st.session_state:
        st.session_state['Outline'] = ""
    if 'OutlineObj' not in st.session_state:
        st.session_state['OutlineObj'] = {}
    if 'sectionsList' not in st.session_state:
        st.session_state['sectionsList'] = []
    if 'finished' not in st.session_state:
        st.session_state['finished'] = False

    # è®¾ç½®æ ‡é¢˜
    st.title('ç®¡ç†ç¡•å£«ä½œä¸šå†™ä½œåŠ©æ‰‹')
    st.subheader('æ­¤åŠ©æ‰‹å¯ä»¥æ ¹æ®ä½ çš„ä½œä¸šæ’°å†™èŒƒå›´åŠæ–¹å‘ï¼Œç”Ÿæˆ5000å­—ä»¥ä¸‹çš„æ–‡å­—å†…å®¹ï¼ˆåŒ…æ‹¬é¢˜ç›®ã€æ­£æ–‡ã€å‚è€ƒæ–‡çŒ®ï¼‰')
    st.subheader('ç›®å‰è¯¥åŠ©æ‰‹ä¸å¯ä»¥åšå®šé‡æ¨¡å‹æ•°æ®ï¼Œç”Ÿæˆéƒ½æ˜¯æ–‡å­—å½¢å¼')
    st.subheader('è¯·æ ¹æ®ä½œä¸šéœ€æ±‚å¡«å†™ä»¥ä¸‹å†…å®¹ï¼Œå¡«å†™è¶Šè¯¦ç»†ï¼Œç”Ÿæˆå†…å®¹è¶Šç¬¦åˆè¦æ±‚')


    # ä¸ºç”¨æˆ·æä¾›è¾“å…¥æ¡†
    st.markdown('##### è¯·è¾“å…¥ä½ çš„ä½œä¸šé¢˜ç›®ï¼›æˆ–è€…è¾“å…¥ä½ çš„ä½œä¸šå†™ä½œæ–¹å‘ï¼Œæˆ‘å¸®ä½ ç”Ÿæˆä½œä¸šé¢˜ç›®ï¼ˆå¿…é¡»å¡«å†™ï¼‰ğŸ“œ')
    title = st.text_input('ä½œä¸šé¢˜ç›®')
    st.markdown('##### ä½œä¸šè¯­è¨€è¦æ±‚ï¼ˆå¿…å¡«ï¼‰')
    language = st.selectbox('ä½œä¸šè¯­è¨€',["ä¸­æ–‡","English"])
    st.markdown('##### ä½œä¸šå­—æ•°è¦æ±‚ï¼ˆå¿…å¡«ï¼‰')
    counts = st.text_input('ä½œä¸šå­—æ•°')
    st.markdown('##### ä½œä¸šç« èŠ‚è¦æ±‚ï¼šâ€œç¬¬ä¸€ç« èŠ‚.....;ç¬¬äºŒç« èŠ‚.....â€ï¼Œè‹¥ä½œä¸šæ²¡æœ‰ç›¸å…³è¦æ±‚ï¼Œåˆ™å¯ä»¥ä¸å¡«å†™ï¼Œæˆ‘å¸®ä½ ç”Ÿæˆç« èŠ‚')
    angle_and_conclusion = st.text_area('ç« èŠ‚ä»‹ç»ï¼ˆå¯é€‰ï¼‰')
    st.markdown('##### ä½œä¸šå…¶ä»–è¦æ±‚ï¼Œåœ¨è¿™é‡Œè¾“å…¥ï¼ˆå¦‚æœæ²¡æœ‰å¯ä»¥ä¸å¡«ï¼‰')
    additional_info = st.text_area('å…¶ä»–è¦æ±‚ï¼ˆå¯é€‰ï¼‰')

    st.subheader('PS:ç”±äºæ¨¡å‹çš„è„±æ•å¤„ç†ï¼Œå‚è€ƒæ–‡çŒ®å¹¶ä¸ä¸€å®šä¸ºçœŸï¼Œä½œä¸šè¦æ±‚æ¯”è¾ƒä¸¥æ ¼çš„æ—¶å€™ï¼Œè¯·è‡ªè¡Œæ·»åŠ å‚è€ƒæ–‡çŒ®')

    # æä¾›ä¸€ä¸ªæŒ‰é’®ï¼Œç‚¹å‡»åç”Ÿæˆå¤§çº²
    if st.button('ç”Ÿæˆæ–‡ç« '):
        st.session_state['Outline'] = ""
        st.session_state['OutlineObj'] = {}
        st.session_state['sectionsList'] = []
        st.session_state['finished'] = False
        if not title or not language or not counts:
            # å¦‚æœç”¨æˆ·åæˆ–å¯†ç ä¸ºç©ºï¼Œåˆ™æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
            st.warning("è¯·å¡«å†™å¿…é¡»å¡«å†™çš„ä¿¡æ¯")
        else:
            messages = []
            GPT_response = st.empty()
            prompt = f"""
            ## ä»»åŠ¡
            æ ¹æ®ä»¥ä¸‹çš„ä¿¡æ¯è¿›è¡Œæ–‡ç« å¤§çº²çš„åˆ›ä½œï¼Œä¸ºè¯¥æ–‡ç« å–ä¸€ä¸ªé†’ç›®çš„åç§°ï¼Œå¹¶ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆä¸€äº›æ¦‚æ‹¬æ€§å†…å®¹
            1. **ä¸»é¢˜å’Œç„¦ç‚¹**ï¼š
                - æ–‡ç« çš„æ ‡é¢˜å’ŒèŒƒå›´ï¼š{title}
                - ä½œä¸šçš„è¦æ±‚ {angle_and_conclusion}
                - æ˜¯å¦æœ‰ä¸€äº›é™„åŠ ä¿¡æ¯ä¸ºæˆ‘æä¾›æœ€æ–°çš„å†…å®¹? {additional_info}
            2. **å†™ä½œä»»åŠ¡**ï¼š
                - æ–‡ç« çš„ç±»å‹ï¼Ÿè¿™æ˜¯ä¸€ç¯‡è®ºæ–‡ä½œä¸š
                - ä½ å¸Œæœ›è®©è¯»è€…é€šè¿‡æ–‡ç« è·å¾—ä»€ä¹ˆæ ·çš„ä½“éªŒæˆ–ä¿¡æ¯ï¼Ÿå¸Œæœ›èƒ½å¤Ÿè®ºè¯è‡ªå·±çš„è§‚ç‚¹
                - **æ–‡ç« è¾“å‡ºçš„è¯­è¨€ä¸º {language}**
                - æ ¹æ®æ–‡ç« çš„å­—æ•°{counts}ï¼Œæ¥æ„å»ºç« èŠ‚ï¼Œ3000ä»¥ä¸Šä¸º3ç« ï¼Œ4000å­—ä»¥ä¸Šä¸º4ç« 
            3. **æ–‡ç« é£æ ¼**ï¼š
                - ä½ å¸Œæœ›æ–‡ç« çš„è¯­è°ƒæˆ–é£æ ¼æ˜¯æ€æ ·çš„ï¼Ÿå­¦æœ¯
            4. **æ ¼å¼å’Œç»“æ„**ï¼š
                - å¼•è¨€ // å¿…é¡»è¦æœ‰ï¼Œå¯¹å…¨æ–‡åšä¸€ä¸ªæ€»ç»“æ€§çš„è¯´æ˜
                - 3-4ä¸ªç« èŠ‚ // å–å†³äº[å†™ä½œä»»åŠ¡]
                - ç»“è®º // å¿…é¡»è¦æœ‰ï¼Œå¯¹å…¨æ–‡çš„ç»“è®ºåšä¸€ä¸ªè¯´æ˜
                - å‚è€ƒæ–‡çŒ® // ä»ä½ çš„è®¤çŸ¥ä¸­ï¼Œè¾“å‡ºä¸å°‘äºå…­ä¸ªå‚è€ƒæ–‡çŒ®ï¼Œå…¶ä¸­è‡³å°‘æœ‰å››ä¸ªæ˜¯ä¸­æ–‡æ–‡çŒ®ï¼Œ
                - å‚è€ƒæ–‡çŒ® // å‚è€ƒå¿…é¡»çœŸå®æœ‰æ•ˆï¼Œå¹¶æœ‰æƒå¨çš„æ¥æº
            """
            # test = f"é²è¿…å’Œå‘¨æ ‘äººçš„å…³ç³»"
            with st.spinner("æ­£åœ¨ç”Ÿæˆå¤§çº²ï¼Œè¯·ç¨å€™..."):
                chat_completion = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡ç« å†™æ‰‹,ä»¥markdownæ ¼å¼è¾“å‡º"},
                        {"role": "user", "content": prompt},
                        ],
                    stream=True,
                )

                for token in chat_completion:
                    content = token.choices[0].delta.content # new API result
                    if content != None:
                        messages.append(content)
                        full_reply_content = ''.join([m for m in messages])
                        GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")

                st.session_state['Outline'] = full_reply_content
                st.rerun()

    if st.session_state['Outline']:
        #st.write(st.session_state['Outline'])
        if st.session_state['finished']:
            st.write(st.session_state['Outline'])
            # st.write(st.session_state['OutlineObj'])
            # st.write(st.session_state['sectionsList'])

            chat_text = ""
            chat_text += f"# {st.session_state['OutlineObj']['title']} \n"
            sectionsList = st.session_state['sectionsList']
            i=0

            while i < len(sectionsList):
                chat_text += f"## {st.session_state['OutlineObj']['sections'][i]} \n"
                chat_text += f"{sectionsList[i]}\n\n"
                i = i+1
            
            references = st.session_state['OutlineObj']['references']
            j=0

            chat_text += f"## å‚è€ƒæ–‡çŒ®\n"
            while j < len(references):
                chat_text += f"{j+1}. {st.session_state['OutlineObj']['references'][j]} \n"
                j = j+1

            st.write(chat_text)

            # ä½¿ç”¨Pypandocè½¬æ¢Markdownå†…å®¹ä¸ºdocxæ ¼å¼
            converted_file = pypandoc.convert_text(
                chat_text,
                'docx',
                format='md',
                outputfile='output.docx',
                extra_args=['--standalone']
            )

            # è¯»å–è½¬æ¢åçš„æ–‡ä»¶å†…å®¹ï¼Œå‡†å¤‡ä¸‹è½½
            with open('output.docx', 'rb') as f:
                docx_file = f.read()

            # åˆ›å»ºä¸€ä¸ªBytesIOæµæ¥å­˜æ”¾docxæ–‡ä»¶
            byte_io = BytesIO(docx_file)

            # æ·»åŠ ä¸‹è½½æŒ‰é’®
            timestamp = datetime.now()
            file_name = f"{st.session_state['OutlineObj']['title']}_{timestamp}.docx"

            # è®¾ç½®streamlitä¸‹è½½æŒ‰é’®ï¼Œå…è®¸ç”¨æˆ·ä¸‹è½½æ–‡ä»¶
            st.download_button(
                label='ä¸‹è½½wordæ–‡æ¡£',
                data=byte_io,
                file_name=file_name,
                mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
            )
        
        else:
            if st.session_state['OutlineObj']:
                # st.write(st.session_state['OutlineObj'])
                st.write("å¼€å§‹æŒ‰ç…§ç« èŠ‚ç›®å½•æ¥è¿›è¡Œå†…å®¹çš„ç”Ÿæˆ")
                title = st.session_state['OutlineObj']["title"]
                sections = st.session_state['OutlineObj']["sections"]
                for section in sections:
                    msg = f"æ‰©å†™æ ‡é¢˜ï¼š{title}ä¸‹çš„ç« èŠ‚:{section},è¯·ç¨å..."
                    with st.spinner(msg):
                        messages = []
                        GPT_response = st.empty()

                        prompt = f"""
                        # MBAä½œä¸šåŠ©æ‰‹-æ‰©å†™
                        ## ä»»åŠ¡
                        1. åŸºäº<å¤§çº²>, ç»“åˆæ ‡é¢˜ {title} ,**ä»…å¯¹ {section} éƒ¨åˆ†è¿›è¡Œæ‰©å†™**ï¼Œéœ€è¦è‡³å°‘1000å­—çš„å†…å®¹ï¼Œè¾“å‡ºçš„è¯­è¨€ä¸º{language}
                        1.1 **æ³¨æ„ä¸éœ€è¦è¾“å‡ºç« èŠ‚çš„æ ‡é¢˜ï¼ï¼**
                        1.2 **æ³¨æ„ä¸éœ€è¦ç›´æ¥è¾“å‡ºå‚è€ƒæ–‡çŒ®ï¼ï¼ä½†éœ€è¦åœ¨åŸæ–‡ä¸­åˆ©ç”¨è§’æ ‡è¿›è¡Œå‚è€ƒæ–‡çŒ®çš„æ ‡æ³¨**
                        1.3 **æ³¨æ„ä¸è¦è¾“å‡º'ç°åœ¨æˆ‘å¯¹è¿™ä¸€æ®µè¿›è¡Œæ‰©å†™'ç±»ä¼¼çš„å­—æ ·**
                        2. ç»“åˆ<æ³¨æ„äº‹é¡¹>è¿›è¡Œå†…å®¹çš„è¾“å‡º
                        ## å¤§çº²
                        {st.session_state['Outline']}
                        ## æ³¨æ„äº‹é¡¹
                        1. **ç›®æ ‡å—ä¼—**ï¼š
                            - è¿™æ˜¯ä¸€ç¯‡è®ºæ–‡ä½œä¸š
                            - å¸Œæœ›èƒ½å¤Ÿè®ºè¯è‡ªå·±çš„è§‚ç‚¹
                        2. **æ–‡ç« é£æ ¼**ï¼š
                            - å­¦æœ¯ //æ³¨æ„ï¼Œä¸è¦ä»¥â€˜æˆ‘ä»¬â€™çš„å£å»è¿›è¡Œè¾“å‡ºï¼Œè¦çªå‡ºä½ çš„è´¡çŒ®
                        3. **å›¾è¡¨æˆ–è§†è§‰å…ƒç´ **ï¼š
                            - åœ¨éœ€è¦è¿›è¡Œæ’å…¥è§†è§‰å…ƒç´ çš„åœ°æ–¹è¿›è¡Œæ ‡æ³¨
                        4. **å‚è€ƒæ–‡çŒ®**ï¼š
                            - **ä¸éœ€è¦è¾“å‡ºå‚è€ƒæ–‡çŒ®ï¼Œä½†éœ€è¦åœ¨åŸæ–‡ä¸­åˆ©ç”¨è§’æ ‡`[x]`çš„æ ¼å¼è¿›è¡Œå‚è€ƒæ–‡çŒ®çš„æ ‡æ³¨**
                            - **ä¸éœ€è¦è¾“å‡ºFootnotesè¿™ç±»çš„éƒ¨åˆ†**
                        """

                        chat_completion = client.chat.completions.create(
                            model="gpt-4-1106-preview",
                            messages=[
                                {"role": "system", "content": ""},
                                {"role": "user", "content": prompt}
                            ],
                            stream=True,
                        )

                        if isinstance(chat_completion, dict):
                            # not stream
                            st.write(chat_completion.choices[0].message.content)
                        else:
                            # stream
                            for token in chat_completion:
                                content = token.choices[0].delta.content # new API result
                                if content != None:
                                    messages.append(content)
                                    full_reply_content = ''.join([m for m in messages])
                                    # GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")

                            st.session_state['sectionsList'].append(full_reply_content)                       
                st.session_state['finished'] = True
                st.rerun()
            
            else:
                with st.spinner("æ­£åœ¨åˆ†æå¤§çº²å†…å®¹ï¼Œè¯·ç¨å€™..."):
                    messages = []
                    GPT_response = st.empty()

                    prompt = f"""
                        # OutlineToJson
                        ## Task
                        * READ THE <outline> and OUTPUT A JSON STRING ONLY!!!.without any markdown symbol
                        * the Json String is DEFINED by <JSON Format>
                        * MAKE SURE your output can be pharsed by json.load() directly!
                        ### JSON Format
                        {{
                            "title": "æ–‡ç« æ ‡é¢˜",
                            "sections": [], //åªéœ€è¦è¿”å›ç« èŠ‚ï¼ˆåŒ…å«å¼•è¨€å’Œç»“è®ºï¼‰çš„titleå³å¯ï¼Œä¸éœ€è¦è¿”å›ç« èŠ‚å†…å®¹ï¼ï¼ï¼
                            "references":[], //å°†å‚è€ƒæ–‡çŒ®ä¿å­˜ä¸ºä¸€ä¸ªåˆ—è¡¨
                        }}
                        ### outline
                        {st.session_state['Outline']}
                    """

                    chat_completion = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        response_format={"type": "json_object"},
                        messages=[
                            {"role": "system", "content": ""},
                            {"role": "user", "content": prompt}
                            ],
                        stream=True,
                    )

                    if isinstance(chat_completion, dict):
                        # not stream
                        st.write(chat_completion.choices[0].message.content)
                    else:
                        # stream
                        for token in chat_completion:
                            content = token.choices[0].delta.content # new API result
                            if content != None:
                                messages.append(content)
                                full_reply_content = ''.join([m for m in messages])
                                GPT_response.markdown(f"ğŸ¤–: {full_reply_content}")
                        # jsonstring=full_reply_content.replace('```json','')
                        st.write(full_reply_content)
                        st.session_state['OutlineObj'] = json.loads(full_reply_content)
                        st.rerun()